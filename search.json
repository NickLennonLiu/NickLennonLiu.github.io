[{"title":"Hello","url":"/2025/05/01/hello/","content":" Hello\nThis is the default note.\n","tags":["Hexo"]},{"title":"MCP Server","url":"/2025/05/04/interview/MCP%20Server/","content":"![[Pasted image 20250430171745.png]]\n以下是 MCP 的基本工作流程：\n\n\n初始化连接：客户端向服务器发送连接请求，建立通信通道。\n\n\n发送请求：客户端根据需求构建请求消息，并发送给服务器。\n\n\n处理请求：服务器接收到请求后，解析请求内容，执行相应的操作（如查询数据库、读取文件等）。\n\n\n返回结果：服务器将处理结果封装成响应消息，发送回客户端。\n\n\n断开连接：任务完成后，客户端可以主动关闭连接或等待服务器超时关闭。\n![[Pasted image 20250430172002.png]]\nMCP 遵循客户端-服务器架构（client-server），其中包含以下几个核心概念：\n\n\nMCP 主机（MCP Hosts）：发起请求的 LLM 应用程序（例如 Claude Desktop、IDE 或 AI 工具）。\n\n\nMCP 客户端（MCP Clients）：在主机程序内部，与 MCP server 保持 1:1 的连接。\n\n\nMCP 服务器（MCP Servers）：为 MCP client 提供上下文、工具和 prompt 信息。\n\n\n本地资源（Local Resources）：本地计算机中可供 MCP server 安全访问的资源（例如文件、数据库）。\n\n\n远程资源（Remote Resources）：MCP server 可以连接到的远程资源（例如通过 API）。\n![[Pasted image 20250430171755.png]]\n\n\n 通信机制\nMCP 协议支持两种主要的通信机制：基于标准输入输出的本地通信和基于SSE（Server-Sent Events）的远程通信。\n这两种机制都使用 JSON-RPC 2.0 格式进行消息传输，确保了通信的标准化和可扩展性。\n\n本地通信**：**通过 stdio 传输数据，适用于在同一台机器上运行的客户端和服务器之间的通信。\n远程通信**：**利用 SSE 与 HTTP 结合，实现跨网络的实时数据传输，适用于需要访问远程资源或分布式部署的场景。\n\n","tags":["llm","agent"]},{"title":"DeepSeek","url":"/2025/05/04/interview/DeepSeek/","content":" 资料\nhttps://www.zhihu.com/tardis/zm/art/19868935152\nhttps://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\n[[DeepSeek_R1.pdf]]\n![[Pasted image 20250220155728.png]]\n研究问题：如何通过[[强化学习]]（RL）有效提升大型语言模型（LLM）的推理能力？\n问题背景：\n\n之前的研究大多依赖于大量的 SFT 数据，但获取高质量的 SFT 数据成本高昂。\nOpenAI 的 o1 系列模型通过增加思维链（Chain-of-Thought, CoT）推理过程的长度来提升推理能力，但如何有效进行测试时（test-time）扩展仍是开放问题。\n一些研究尝试使用基于过程的奖励模型（PRM）、强化学习和搜索算法（MCTS）来解决推理问题，但没有达到 OpenAI 的 o1 系列模型的通用推理性能水平。\n论文动机： 探索是否可以通过纯强化学习来让 LLM 自主发展推理能力，而无需依赖 SFT 数据。\n\n R1的多阶段训练策略\n 阶段1： 有监督微调SFT（冷启动）\n使用少量高质量的 CoT 数据进行冷启动，预热模型。\nDeepSeek-R1 使用冷启动数据的主要目的是为了解决 DeepSeek-R1-Zero 在训练早期出现的训练不稳定问题。相比于直接在基础模型上进行 RL，使用少量的 SFT 数据进行冷启动，可以让模型更快地进入稳定训练阶段：\n\n可读性：冷启动数据使用更易于理解的格式，输出内容更适合人类阅读，避免了 DeepSeek-R1-Zero 输出的语言混合、格式混乱等问题。\n潜在性能：通过精心设计冷启动数据的模式，可以引导模型产生更好的推理能力。\n稳定训练：使用 SFT 数据作为起始点，可以避免 RL 训练早期阶段的不稳定问题。\n\n为了获取这些数据，deepseek 探索了几种策略：利用长思维回答作为 few-shot 示例，直接提示模型生成包含反思和验证步骤的详细答案，以及收集 DeepSeek-R1-Zero 的输出并通过人工标注者进行细化。最终收集了数千条冷启动数据，用以微调 DeepSeek-V3-Base 作为 RL 训练的起点。DeepSeek-R1 创建的冷启动数据采用了一种可读模式，明确将输出格式定义为：|special_token|&lt;reasoning_process&gt;|special_token|&lt;summary_&gt;。\n 阶段2：RL（基于规则奖励）\n(R1-Zero)规则奖励：\n\n准确率奖励：准确率奖励模型评估响应是否正确。例如，在具有确定性结果的数学问题中，模型需要以指定的格式（box）提供最终答案，从而能够通过基于规则的验证来可靠地确认正确性。同样，对于 LeetCode 问题，可以使用编译器根据预定义的测试用例生成反馈。\n格式奖励: 除了准确性奖励模型，还采用了一种格式奖励模型，要求模型将其思考过程放在 ‘’ 和 ‘’ 标签之间。\n\n在R1的训练时，引入语言一致性奖励，该奖励根据思维链（CoT）中目标语言单词的比例来计算，以减少推理过程中的语言混合问题。\n尽管消融实验表明，语言一致性奖励会导致模型性能略有下降，但它更符合人类的偏好，提高了内容的可读性。最终，通过将推理任务的准确性与语言一致性奖励直接相加，形成了综合的奖励函数。随后，对微调后的模型进行了强化学习（RL）训练，直至其在推理任务上达到收敛。\n 阶段3：拒绝采样和两轮SFT（CoT数据60w+通用数据20w）\n使用上一阶段的 RL 模型进行拒绝采样，生成高质量的推理和非推理数据，并用这些数据对模型进行微调。侧重点是提升模型的综合能力，使其在写作、事实问答等多种任务上表现良好。\n当 RL 训练接近收敛时，使用中间的 checkpoint 来采样监督微调（SFT）数据。与初期主要关注推理能力的冷启动数据不同，这一阶段加入了其他领域的数据，旨在增强模型在写作、角色扮演以及其他通用任务上的表现。具体的数据生成和模型微调步骤如下：\n\n对于推理数据，构建推理 prompt，并从上述 RL 训练的 checkpoint 中进行拒绝采样，以生成推理轨迹。在之前的阶段，仅使用了基于规则的奖励来评估数据。然而，在这个阶段，通过添加其他数据来丰富数据集，其中部分数据使用了生成奖励模型，通过将真实值和模型预测输入 DeepSeek-V3 进行判断。同时，为了提升数据质量，过滤掉混合语言、长段落和代码块的思维链。对于每个提示，采样多个响应，并仅保留正确的响应。最终，收集了大约60万个与推理相关的训练样本。\n对于非推理数据，如写作、问答、翻译等任务，使用 DeepSeek-V3 SFT 数据集的一部分。对于简单的 query，如“你好”，不使用思维链作为回答。经过筛选和整理，最终收集了大约20万个与推理无关的训练样本。\n\n最终，使用大约80万个样本（60w推理+20w通用）对 DeepSeek-v3-Base 模型进行了两轮的 SFT。\n 阶段4：全场景强化学习（规则奖励+奖励模型）\n在上一阶段 SFT 模型的基础上进行 RL 训练，侧重点是使模型在所有场景下都能表现良好，包括推理任务和非推理任务，并且保证模型的安全性和无害性。\n R1-Zero的Aha Moment\n在大规模强化学习中，模型的「思考过程」会不断与最终的正确率奖励相互作用。当模型最初得出的答案并未得到较高奖励时，它会在后续的推理中「回头反省」，尝试补充或修正先前的思路，从而获得更高的奖励。随着强化学习的迭代，这种「主动回溯、推翻先前想法并重新推理」的行为逐渐巩固，便在输出中表现为所谓的「aha moment」。本质上，这是 RL 为模型「留出了」足够的思考和试错空间，当模型自行发现更优思路时，就会出现类似人类「恍然大悟」的瞬间。\n 一些问题\n 为什么在蒸馏到小模型时，直接用 RL 在小模型上训练不如先做大模型再蒸馏？\n大模型在 RL 阶段可能出现许多高阶推理模式。而小模型因为容量和表示能力有限，很难在无监督或纯 RL 情境下学到相似水平的推理模式。\n蒸馏可将「大模型的推理轨迹」直接转移给小模型，小模型只需要模仿大模型相对完备的推理流程，可以在较小训练/推理开销下取得远胜于自身独立强化学习的效果。\n在蒸馏模型的实现中，仅采用了 SFT 阶段，而未包含 RL 阶段，尽管 RL 的加入能显著提升模型性能。按照 deepseek 的说法，本工作的核心目的在于展示蒸馏技术的有效性，而将 RL 阶段的深入探索留给更广泛的研究社群去完成。\n 为什么 PRM 和 MCTS 没有成功？\n论文中提到，基于过程奖励模型（PRM）和蒙特卡洛树搜索（MCTS）并不适合 LLM 的推理。\n PRM 的挑战\n\n难以定义通用的、细粒度的推理步骤。\n难以准确判断中间步骤的正确性，且自动标注方法效果不佳，人工标注又难以扩展。\n模型化的 PRM 容易导致奖励黑客（Agent 利用奖励函数或环境中的漏洞来获取高奖励，而并未真正学习到预期行为。）行为，并且会增加额外的训练成本。\n\n MCTS 的挑战\n\nLLM 的 token 生成搜索空间巨大，远远超出棋类游戏，容易陷入局部最优解。\n价值模型的训练非常困难，导致难以迭代提升。\n\n 结论\n\n第一，将更强大的模型蒸馏到较小模型中能产生优异的结果，而依赖大规模强化学习的小模型则需消耗巨大的计算资源，且可能仍无法达到蒸馏所能达到的性能水平。\n第二，尽管蒸馏策略既经济又高效，但若要突破智能的界限，可能仍需依赖更强大的基础模型以及大规模强化学习。\n\n","tags":["llm"]},{"title":"BGE - Embedding模型","url":"/2025/05/04/interview/BGE%20-%20Embedding%E6%A8%A1%E5%9E%8B/","content":"BGE的目标是 做中文世界的通用embedding模型 。\n通用，意味着 用一个模型，支持所有的embedding使用场景 ，包括但不限于：retrieval、re-rank、clustering、classification、pair-classification等任务。\nBGE从两个方面来达成这个目标：\n\n数据方面 ，兼顾 scale、 diversity、 quality这三个维度，这是通用embedding模型能训练出来的前提 ；\n训练策略方面 ，论文使用3阶段训练策略，从 pre-training 到 general-purpose fine-tuning 再到 task-specific fine-tuning；前两个阶段是保证通用性的基石， 最后一个阶段则在保持通用 的基础上，进一步精进 下游任务的效果。\n\n 数据\n在 训练数据 方面，论文构建了大量的text pair数据，论文称之 C-MTP(Chinese Massive Text Pairs)，数据量总计 100M ，涵盖多种任务，来自Wudao[3]等开源数据集，结合一些filter策略，同时达到scale、diversity、quality三个目标。\n具体而言， C-MTP分成unlabeled和labeled两部分。\n\nunlabeled 数据，源于open web content和public Chinese dataset。前者包括 Wudao Corpora、知乎、百科等数据，使用(title, passage)作为text pair；后者包括CSL、CMRC等公开数据集，这些数据集中pair结构天然存在，因此直接使用；同时，为了保证数据quality，使用Text2Vec-Chinese（预训练的中文embedding模型）[4]，过滤掉得分_低于0.43_的pair数据。最终数据量共计100M；\nlabeled 数据，直接来自于下游任务的标注数据集，包括DuReader、mMARCO、NLI-Zh等，涵盖retrieval、ranking、similarity comparison等任务，数据量共计838K。\n\n在 测试数据 方面，论文构建了中文世界的benchmark，称之为 C-MTEB(Chinese Massive Text Embedding Benchmark)。\n 训练方面\n\n简要来说， pre-training阶段在 Wudao Corpora上进行，此阶段未在任何pair数据上训练，其目标是训练出更适合embedding任务的 pre-trained model ；\ngeneral-purpose fine-tuning阶段在 C-MTP(unlabeled)上进行，该阶段在100M的text pairs上训练，可以视作一种 大规模的弱监督学习 过程，可初步学习出通用embedding model；\n最后的 task-specific fine-tuning阶段，在 C-MTP(labeled)上进行，通过在少而精的下游任务labeled data上微调，在 保证通用性 的同时，强化模型在 具体任务 上的表现。\n 训练细节\n pre-training阶段\n前文提到，这一阶段是为了学习出更适合embedding的pre-trained model。\n论文采取了RetroMAE[5]的训练策略。其目标函数如下：\n\n简单来说，就是先对text X进行随机Mask，然后进行encoding，再额外训练一个light-weight decoder（如单层transformer）进行重构。通过这一过程，强迫encoder学习到良好的embedding。\n一个很自然的疑问是， 这种方法比Bert要好吗？\n\n作者对此进行了实验。其中 BGE-i w.o. pre-train模型直接使用了Chinese-RoBERTa[6]，与本文的 BGE-i模型对比，可以发现 整体表现其实差不多 ，只是在 retrieval任务 上有较明显的提升。\n\nretroMAE的提出也正是为了增强retrieval任务的表现。\n\n general-purpose fine-tuning阶段\n这一阶段的核心技术是对比学习，重点是：\n\n采用in-batch negative sample方法；\n使用大batch_size（论文使用的size为19200）。\n\n这一阶段主打一个简单粗暴 – 只要batch够大，在batch内就足以找到hard negative sample 。\n\n论文实验表明， BGE-i（仅经过general-purpose fine-tuning）的整体表现就 已经可以超过 OpenAI-Ada-002和 M3E（large）了，细看一番，其中 优势最大的是retrieval任务 ，这与其训练数据（ C-MTP-unlabel）中包含大量web content中的（title，passage）有很大的关系。\n\n着重优化retrieval任务的表现，应该是BGE团队的刻意为之；毕竟retrieval是embedding model最常见的使用场景。\n\n task-specific fine-tuning阶段\n这一阶段的难点在于： 在任务间存在差异的情况下，如何更好地multi-task learning。\n论文采取了两个关键技术：\n\ninstruction-based fine-tuning[7]。核心思路是将衡量 sim（x1，x2），转化为衡量 sim（instruction+x1，instruction+x2），这个instruction就是一段text prompt，用以说明domain、task等内容。例如在retrieval任务中，query侧加入的instruction为 为这个句子生成表示以用于检索相关文章：；\nhard negative sampling 。在训练过程中，采取ANN-style sampling strategy[8]，从该任务的corpus中全局性 地采样出一个embedding表征最接近的hard negative sample 。\n\n\n论文实验表明， BGE-f（经过task-specific fine-tuning之后的最终模型），在retrieval、STS、pair-classification、re-rank上 明显好于 BGE-i模型，其他任务也 几乎没有效果损失 。\n这充分说明了论文所采取的fine-tuning技术的有效性 。\n可以认为，这也是BGE团队做出的fine-tuning示范。\n BGE的效果如何\nBGE所采取的模型结构类似于BERT，使用其最后一层中 CLS token的hidden state作为embedding。\nBGE模型有3个不同大小的版本，其中small版参数量为 24M 、base版参数量为 102M 、large版参数量为 326M 。\n论文在 C-MTEB上与众多embedding模型进行了对比，结果如下。\n\n可以看到， BGE-base和BGE-large基本是一骑绝尘 ，几乎在每一个任务上的效果都明显更好， 即使是BGE-small，也几乎能达到SOTA的效果 。\n 总结\n本文对BGE的论文进行了简要解读，指出其在数据侧、训练侧所采取的方法论，同时对其三阶段训练过程的技术细节进行了介绍。\n一个很明显的感受： BGE是一个产品 ，它做到的是集各家之所长（数据、训练技术等），产品的诞生便是它的创新之处。从论文实验来看，在中文的各项任务上，就算是 small-size BGE，也可以战胜 OpenAI-Ada-002，而且BGE是开源的 – 这意味着每个人都可以对它再次进行fine-tuning，以进一步提升效果。这的确是中文开发者的福音。\n","tags":["NLP"]},{"title":"Qwen3","url":"/2025/05/04/interview/Qwen3/","content":"\nDense and Mixture-of-Experts\n无缝切换：思考模式和飞思考模式\n推理能力加强\n更好的人类偏好对齐\nagent能力\n100+语言和方言\n\n 模型\n\nQwen3-235B-A22B：235B总参数和22B激活参数\n\n48层，Q:32，KV:4，Experts（total/activated): 128 / 8，Context Length: 128K\n\n\nQwen3-30B-A3B：30B总参数和3B激活参数\nQwen3-32/14/8/4/1.7/0.6B\n\n 预训练\n在36T token上去训练，是Qwen2.5的两倍（18T）\n来源：web, pdf, 用Qwen2.5-VL提取文档中的文本并用Qwen2.5提升质量\n预训练过程共分为三个阶段。\n在第一阶段（S1），模型在超过30T个token上进行预训练，使用的上下文长度为4K标注符。这一阶段使模型掌握了基本的语言能力和通用知识。\n在第二阶段（S2），我们优化了训练数据集，提高了知识密集型数据的占比，例如STEM领域、编程和推理任务。随后模型在额外的5T标注符号上继续进行预训练。\n在最后阶段，我们采用高质量的长上下文数据，将上下文长度扩展至32K标注符号，以确保模型能够有效处理更长的输入内容。\n Post-Training\n![[Pasted image 20250430170954.png]]\n为了开发一个既具备逐步推理能力又能快速响应的混合模型，我们设计并实施了一个四阶段的训练流程，具体包括：(1) 长链式思维（CoT）冷启动，(2) 基于推理的强化学习（RL），(3) 思维模式融合，以及 (4) 通用强化学习。\n在第一阶段，我们使用涵盖数学、编程、逻辑推理和STEM问题等多种任务和领域的多样化长链式思维数据对模型进行微调，旨在赋予模型基本的推理能力。第二阶段重点在于扩大强化学习所使用的计算资源，通过基于规则的奖励机制提升模型的探索与利用能力。\n第三阶段通过在长链式思维数据和常规指令微调数据的组合上进行微调，将“非思维”能力融合进已具备推理能力的模型中。这些数据由第二阶段强化后的思维模型生成，从而实现推理与快速响应能力的自然融合。最后，在第四阶段，我们在20多个通用领域任务上应用强化学习，以进一步增强模型的通用能力，并纠正不期望的行为。这些任务包括指令遵循、格式遵循、代理执行能力等。\n![[Pasted image 20250430171232.png]]\n Future work\nQwen3 是我们迈向通用人工智能（AGI）与超级人工智能（ASI）进程中的重要里程碑。通过扩大预训练与强化学习（RL）的规模，我们实现了更高层次的智能水平。同时，Qwen3 实现了思维模式与非思维模式的无缝融合，允许用户灵活控制推理资源预算。此外，我们还拓展了对多种语言的支持，显著提升了全球可达性。\n展望未来，我们将从多个维度持续优化模型，重点包括改进模型架构与训练方法，以实现若干关键目标：扩大数据规模、增加模型容量、延长上下文长度、拓展多模态能力，并推动融合环境反馈的强化学习，以支持更长跨度的推理。我们相信，行业正从“训练模型”的阶段过渡到“训练智能体”的新阶段，而我们的下一代模型将为每个人的工作与生活带来实质性的进步。\n![[Pasted image 20250430171715.png]]\n","tags":["llm"]},{"title":"大模型","url":"/2025/05/04/interview/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B8%B8%E8%A7%81%E9%9D%A2%E7%BB%8F/","content":"[toc]\n DeepSeek\n[[DeepSeek]]\n 拒绝采样\n[[概率论#LLM 的拒绝采样]]\n 显存计算\n 推理\n\nint8： 模型显存 = 1*参数量（Byte）\nfp16, bf16: 模型显存 = 2*参数量（Byte）\nfp32: 模型显存 = 4*参数量（Byte）\n\n混合精度（Mixed-precision）最后存储的类型也是fp32，公式也适合混合精度。\n 训练\n训练显存消耗：模型参数(Model) + 优化器状态(Optimizer status) + 梯度 (Gradient) + 激活值 (Activation)\n静态值：模型参数和优化器状态\n动态值：激活值和梯度值\n下面默认参数都是4Byte\n 模型参数\n同推理\n 优化器状态\n常见优化器Adam：Momentum和一个Variance，混合精度训练中再加一个模型参数副本\n\n模型副本 4Bytes （副本存32位的）\nMomentum 4Byte\nVariance 4Byte\n\n那就是参数副本+momentum+variance = 32 * 3\n 梯度值\n梯度的数据类型和模型参数一直，因此和模型参数同样规模\n 激活值\n激活值的大小跟模型参数、重计算、并行策略等相关，这里我们参考Megtron论文里面给的计算公式，来求解激活值所占用的显存大小。\ns∗b∗h∗(34+5∗a∗s/h)∗L∗γs*b*h*(34+5*a*s/h) *L *\\gamma \ns∗b∗h∗(34+5∗a∗s/h)∗L∗γ\n\ns 序列长度（sequence length), tokens的量\nb 微批量大小（microbatch size）\nh 隐藏层大小（hidden dimension size）\na attention的头数 （number of attention heads）\nL transformer模型的层数\nλ 比例系数，当为fp16时 值等于1 / (1024 * 1024 * 1024)。\n\n如果全部重计算：\ns∗b∗h∗(2)∗L∗γs*b*h*(2)*L*\\gamma\ns∗b∗h∗(2)∗L∗γ\n 单卡与并行\nTotalMemory=Model+Optimizer+Activation+Gradient\\text{TotalMemory} = \\text{Model} + \\text{Optimizer} + \\text{Activation} + \\text{Gradient}\nTotalMemory=Model+Optimizer+Activation+Gradient\n\nTP: tensor并行：Megatron-LM\nSP: 序列并行\nPP: pipeline并行\nZero：参数服务器\n\nZero1：优化器参数分散到N张卡\nZero2：优化器和梯度分散到N张卡\nZero3：优化器、梯度、模型都分散到N张卡，并且还要算上新引入的参数LiveParams，用于决定哪些参数加载到GPU中\n\n\n\nTotalMemory=Model/(PP∗TP)+Optimizer/N+Activation/TP+Gradient/PP\\text{TotalMemory} = \\text{Model}/(PP*TP) + \\text{Optimizer}/N + \\text{Activation}/TP + \\text{Gradient}/PP\nTotalMemory=Model/(PP∗TP)+Optimizer/N+Activation/TP+Gradient/PP\n LORA\nLora方法的核心是在大型语言模型上对指定参数增加额外的低秩矩阵，也就是在原始PLM旁边增加一个旁路，做一个降维再升维的操作。并在模型训练过程中，固定PLM的参数，只训练降维矩阵A与升维矩阵B。\nPtuning方法的核心是使用可微的virtual token替换了原来的discrete tokens，且仅加入到输入层，并使用prompt\n Stable Diffusion原理\n[[Stable Diffusion]]\n\nTextEncoder\n\nCLIP\n\n\nDiffusion\nVAE\n\n 大模型的模型架构\n\nOnly-encoder\nOnly-decoder\nEncoder-Decoder\n\n","categories":["interview"],"tags":["llm"]},{"title":"NLP","url":"/2025/05/04/interview/NLP/","content":"1.1 Word2Vec（CBOW &amp; Skip-gram）\n概念：\nWord2Vec 是一种将单词映射到向量空间的技术，由 Google 在 2013 年提出。它主要有两种训练方式：\n• CBOW（Continuous Bag of Words）：用上下文词预测目标词，适用于数据较少的情况。\n• Skip-gram：用目标词预测上下文词，在大规模数据上表现更好。\n面试考点：\n\nWord2Vec 为什么能捕捉语义？\n• 因为它通过上下文关系学习分布式词表示，相似的词会被映射到相近的向量空间。\nWord2Vec 的缺点？\n• 不能处理多义词（polysemy），比如 “bank”（银行/河岸）只有一个向量表示。\n• 无法处理 OOV（未登录词），需要提前构建词表。\n如何优化 Word2Vec？\n• 负采样（Negative Sampling）：加速训练并减少计算复杂度。\n• 层次 Softmax（Hierarchical Softmax）：优化计算效率。\n\n"},{"title":"GRPO","url":"/2025/05/04/interview/GRPO/","content":"![[Pasted image 20250430172312.png]]\n","tags":["llm","RL"]},{"title":"反向提问","url":"/2025/05/04/interview/%E5%8F%8D%E5%90%91%E6%8F%90%E9%97%AE/","content":" 反向面试\n\n大部分翻译自：https://github.com/viraptor/reverse-interview ，亦有其他网友补充。\n\n\n译者总结的一份适合突击记忆的简洁版 LeetCode 题解和面试问题，也欢迎 Star。https://github.com/yifeikong/interview\n\n下面列表里的问题对于参加技术面试的人来说可能有些用。\n列表里的问题并不一定适用于某个特定的职位或者工作类型，也没有排序\n最开始的时候这只是我自己的问题列表，但是慢慢地添加了一些我觉得可能让我对这家公司亮红牌的问题。\n我也注意到被我面试的人提问我的问题太少了，感觉他们挺浪费机会的。\n如果你问过的问题没有被列出来，请提交一个 PR。\n翻译：\nEnglish\nKorean\nPortuguese\n繁體中文\n 预期使用方式\n\n检查一下哪些问题你感兴趣\n检查一下哪些是你可以自己在网上找到答案的\n找不到的话就向面试官提问\n\n绝对不要想把这个列表里的每个问题都问一遍。（尊重面试官的时间，而且你可以通过查找已经发布的答案来显示\n你的主动性）\n请记住事情总是灵活的，组织的结构调整也会经常发生。拥有一个 bug 追踪系统并不会保证高效处理 bug。\nCI/CD （持续集成系统） 也不一定保证交付时间会很短。\n 职责\n\nOn-call （电话值班）的计划或者规定是什么？值班或者遇到问题加班时候有加班费吗？\n我的日常工作是什么？\n有给我设定的特定目标吗？\n团队里面初级和高级工程师的比例是多少？（有计划改变吗）\n入职培训 (onboarding) 会是什么样的？\n每个开发者有多大的自由来做出决定？\n在你看来，这个工作做到什么程度算成功？\n你期望我在最初的一个月 / 三个月能够完成什么？\n试用期结束的时候，你会怎么样衡量我的绩效？\n自己单独的开发活动和按部就班工作的比例大概是怎样的？\n一个典型的一天或者一周的工作是怎样安排的？\n对我的申请你有什么疑虑么？\n在这份工作上，我将会和谁紧密合作？\n我的直接上级他们的上级都是什么样的管理风格？（事无巨细还是着眼宏观）\n我在这个岗位上应该如何发展？会有哪些机会？\n每天预期 / 核心工作时间是多少小时？\n我入职的岗位是新增还是接替之前离职的同事？（是否有技术债需要还）？(zh)\n入职之后在哪个项目组，项目是新成立还是已有的？(zh)\n\n 技术\n\n公司常用的技术栈是什么？\n你们怎么使用源码控制系统？\n你们怎么测试代码？\n你们怎么追踪 bug?\n你们怎样监控项目？\n你们怎么集成和部署代码改动？是使用持续集成和持续部署吗 (CI/CD)？\n你们的基础设施搭建在版本管理系统里吗？或者是代码化的吗？\n从计划到完成一项任务的工作流是什么样的？\n你们如何准备故障恢复？\n有标准的开发环境吗？是强制的吗？\n你们需要花费多长时间来给产品搭建一个本地测试环境？（分钟 / 小时 / 天）\n你们需要花费多长时间来响应代码或者依赖中的安全问题？\n所有的开发者都可以使用他们电脑的本地管理员权限吗？\n介绍一下你们的技术原则或者展望。\n你们的代码有开发文档吗？有没有单独的供消费者阅读的文档？\n你们有更高层次的文档吗？比如说 ER 图，数据库范式\n你们使用静态代码分析吗？\n你们如何管理内部和外部的数字资产？\n你们如何管理依赖？\n公司是否有技术分享交流活动？有的话，多久一次呢？(zh)\n你们的数据库是怎么进行版本控制的？(zh)\n业务需求有没有文档记录？是如何记录的？(zh)\n你们是如何面对和解决技术债的？是否有专门的时间或者预算用于重构？\n你们如何进行单元测试呢，是否都有单元测试的习惯?\n\n 团队\n\n工作是怎么组织的？\n团队内 / 团队间的交流通常是怎样的？\n你们使用什么工具来做项目组织？你的实际体会是什么？\n如果遇到不同的意见怎样处理？\n谁来设定优先级 / 计划？\n如果团队没能赶上预期发布日期怎么办？\n每周都会开什么类型的会议？\n会有定期的和上级的一对一谈话吗？\n产品 / 服务的规划是什么样的？（n 周一发布 / 持续部署 / 多个发布流 / …)\n生产环境发生事故了怎么办？是否有不批评人而分析问题的文化？\n有没有一些团队正在经历还尚待解决的挑战？\n你们如何跟踪进度？\n预期和目标是如何设定的？谁来设定？\nCode Review 如何实施？\n给我介绍下团队里一个典型的 sprint\n你们如何平衡技术和商业目标？\n你们如何共享知识？\n团队有多大？\n公司技术团队的架构和人员组成？(zh)\n团队内开发、产品、运营哪一方是需求的主要提出方？哪一方更强势？(zh)\n\n 问未来的同事\n\n开发者倾向于从哪里学习？\n你对在这里工作最满意的地方是？\n最不满意的呢？\n如果可以的话，你想改变哪里？\n团队最老的成员在这里多久了？\n在小团队中，有没有出现成员性格互相冲突的情况？最后是如何解决的？\n\n 公司\n\n公司为什么在招人？（产品发展 / 新产品 / 波动…)\n有没有会议 / 旅行预算？使用的规定是什么？\n晋升流程是怎样的？要求 / 预期是怎样沟通的？\n绩效评估流程是怎样的？\n技术和管理两条职业路径是分开的吗？\n对于多元化招聘的现状或者观点是什么？\n有公司级别的学习资源吗？比如电子书订阅或者在线课程？\n有获取证书的预算吗？\n公司的成熟度如何？（早期寻找方向 / 有内容的工作 / 维护中 / …)\n我可以为开源项目做贡献吗？是否需要审批？\n你认为公司未来五年或者十年会发展成什么样子？\n公司的大多数员工是如何看待整洁代码的？\n你上次注意到有人成长是什么时候？他们在哪方面成长了？\n在这里成功的定义是什么？如何衡量成功？\n有体育活动或者团建么？\n有内部的黑客马拉松活动吗？\n公司支持开源项目吗？\n有竞业限制或者保密协议需要签吗？\n你们认为公司文化中的空白是什么？\n能够跟我说一公司处于不良情况，以及如何处理的故事吗？\n您在这工作了多久了？您觉得体验如何？(zh)\n大家为什么会喜欢这里？(zh)\n公司的调薪制度是如何的？(zh)\n公司有没有申请调岗的制度？\n公司对于员工的心理健康和福祉有什么具体措施？\n你对在这里工作最满意的地方是？你为什么留在这家公司？\n\n 社会问题\n\n你们关于多元化招聘什么看法？\n你们的公司文化如何？你认为有什么空白么？\n这里的工作生活平衡地怎么样？\n公司对气候变化有什么态度吗？\n\n 冲突\n\n不同的意见如何处理？\n如果被退回了会怎样？（“这个在预计的时间内做不完”）\n当团队有压力并且在超负荷工作的时候怎么处理？\n如果有人注意到了在流程或者技术等其他方面又改进的地方，怎么办？\n当管理层的预期和工程师的绩效之间有差距的时候如何处理？\n能给我讲一个公司深处有毒环境以及如何处理的故事吗？\n如果在公司内你的同事因涉嫌性侵犯他人而被调查，请问你会如何处理？\n假设我自己很不幸是在公司内被性侵的受害者，在公司内部有没有争取合法权益的渠道？\n\n 商业\n\n你们现在盈利吗？\n如果没有的话，还需要多久？\n如果有的话，年度营业额是大概有多少？（我现在的公司年度营业额是 5 亿）\n公司的资金来源是什么？谁影响或者制定高层计划或方向？\n你们如何挣钱？\n什么阻止了你们挣更多的钱？\n公司未来一年的增长计划怎样？五年呢？\n你们认为什么是你们的竞争优势？\n你们的竞争优势是什么？\n公司未来的商业规划是怎样的？有上市的计划吗？(zh)\n都在做副业吗？\n\n 远程工作\n\n远程工作和办公室工作的比例是多少？\n公司提供硬件吗？更新计划如何？\n使用自己的硬件办公可以吗？现在有政策吗？\n额外的附件和家具可以通过公司购买吗？这方面是否有预算？\n有共享办公或者上网的预算吗？\n多久需要去一次办公室？\n公司的会议室是否一直是视频会议就绪的？\n\n 办公室布局\n\n办公室的布局如何？（开放的 / 小隔间 / 独立办公室）\n有没有支持 / 市场 / 或者其他需要大量打电话的团队在我的团队旁边办公？\n\n 终极问题\n\n该职位为何会空缺？\n公司如何保证人才不流失？\n这份工作 / 团队 / 公司最好和最坏的方面是？\n你最开始为什么选择了这家公司？\n你为什么留在这家公司？\n\n 待遇\n\n如果有奖金计划的话，奖金如何分配？\n如果有奖金计划的话，过去的几年里通常会发百分之多少的奖金？\n有五险一金(zh)/401k(us)或者其他退休养老金等福利吗？\n五险一金中，补充公积金一般交多少比例？/401k一般交多少比例？我可以自己选择这一比例吗？\n有什么医疗保险吗？如果有的话何时开始？\n有额外商业保险吗？例如人寿保险和额外的养老/医疗保险？\n商业保险可以给家人办理吗？成年人/未成年人？\n更换工作地点，公司付费吗？\n是否可以申请更换工作地点？\n是否愿意协助海外应聘者申请工作签证？\n\n 休假\n\n带薪休假时间有多久？\n病假和事假是分开的还是一起算？\n我可以提前使用假期时间吗？也就是说应休假期是负的？\n假期的更新策略是什么样的？也就是说未休的假期能否滚入下一周期\n照顾小孩的政策如何？\n无薪休假政策是什么样的？\n学术性休假政策是怎么样的？\n孕产假政策具体是怎样的？\n\n 福利\n\n公司提供 mac 开发吗？\n使用自带电脑有补贴吗？\n公积金多少比例缴纳？\n公司是否有食堂，是否有餐饮福利补贴？\n是否提供租房补贴？\n是否提供话费补贴？\n是否有交通补贴？\n\n 人才培养\n\n升职加薪条件是否量化?\n每年给团队安排多少费用用于学习培训?\n每年组织多少次关于技术能力提升的讲座/论坛？\n\n 其他资源\nFind more inspiration for questions in:\n\nThe Joel Test: 12 Steps to Better Code by Joel Spolsky\nQuestions I’m asking in interviews by Julia Evans\n\n License\n\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\n","categories":["interview"],"tags":["interview"]},{"title":"概率论","url":"/2025/05/04/interview/%E6%A6%82%E7%8E%87%E8%AE%BA/","content":" 拒绝采样\n\n LLM 的拒绝采样\nLLM 的拒绝采样操作起来非常简单：让自己的模型针对 prompt 生成多个候选 response，然后用 reward_model 筛选出来高质量的 response （也可以是 pair 对），拿来再次进行训练。\n解剖这个过程：\n\n提议分布是我们自己的模型，目标分布是最好的语言模型；\nprompt + response = 一个采样结果；\ndo_sample 多次 = 缩放提议分布（也可以理解为扔多次骰子）；\n采样结果得到 reward_model 的认可 = 符合目标分布。\n\n经过这一番操作，我们能获得很多的训练样本，“这些样本既符合最好的语言模型的说话习惯，又不偏离原始语言模型的表达习惯 ”，学习它们就能让我们的模型更接近最好的语言模型。\n","categories":["interview"],"tags":["math","probablistics"]},{"title":"机器学习","url":"/2025/05/04/interview/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/","content":"\n激活函数有哪些？\n\n\nsigmoid\n\n值域(0,1)\n梯度消失\n二分类\n\n\n双曲正切tanh\n\n(-1,1)\n比sigmoid梯度更大，收敛更快\n梯度消失\n\n\nReLU\n\n计算简单，收敛快\n可能出现神经元死亡\n\n\nLeakyReLU\n\n 1. 了解 Random Walk 吗？图怎么得到 embedding？\n回答： Random Walk 是图中随机游走的一种方法，常用于图的表示学习。它的基本思想是在图上随机选择一条边走到另一个节点，然后反复进行。\n得到图嵌入（Embedding）：\n\nNode2Vec 和 DeepWalk 是基于 Random Walk 的经典图嵌入方法。它们通过在图上进行随机游走，然后使用 Skip-Gram 模型（类似 Word2Vec）来训练每个节点的嵌入表示。\n\nNode2Vec 改进了传统的深度游走，通过调整游走策略（深度优先和广度优先）来捕获不同层次的节点信息。\nDeepWalk 通过类似 Word2Vec 的模型（CBOW 或 Skip-gram）来学习节点的嵌入。\n\n\n\n","tags":["ML"]},{"title":"问答平台：问题用户匹配（如何找到合适的作答者？）","url":"/2025/05/04/interview/%E9%97%AE%E7%AD%94%E5%B9%B3%E5%8F%B0%EF%BC%9A%E9%97%AE%E9%A2%98%E7%94%A8%E6%88%B7%E5%8C%B9%E9%85%8D%EF%BC%88%E5%A6%82%E4%BD%95%E6%89%BE%E5%88%B0%E5%90%88%E9%80%82%E7%9A%84%E4%BD%9C%E7%AD%94%E8%80%85%EF%BC%9F%EF%BC%89/","content":"\n我面试的团队是想做一个类似知乎的问答类平台，然后给定一个问题，如何从微信用户或者微信公众号之类的内容产出者中找出能够回答这个问题的匹配用户？请你帮我构思一个方案\n\n这是一个 专家用户匹配 的问题，核心在于 构建用户画像 并 匹配问题和用户的兴趣/知识领域。下面是一个完整的方案，涵盖数据收集、用户建模、问题匹配以及系统优化。\n方案：基于微信生态的问答匹配系统\n\n数据收集\n\n1.1 微信用户数据\n可以从以下几个方面获取用户的内容偏好和专业领域：\n•\t公众号订阅情况：用户关注的公众号可以反映其兴趣领域（例如关注“Python之禅”可能是程序员）。\n•\t朋友圈/文章分享：用户经常分享的文章主题，可能表明其知识领域。\n•\t微信读书数据（若可用）：书籍阅读情况可提供更精准的用户画像。\n•\t历史问答或讨论记录（社群）：在微信群、知乎等地方发表的内容可以作为参考。\n1.2 公众号数据\n如果要匹配公众号的作者或内容生产者，可以抓取：\n•\t公众号文章标题 &amp; 关键词：提取文章主题关键词，归类到不同领域。\n•\t文章正文：用 NLP 方法提取核心主题，建立索引。\n•\t作者信息：公众号运营者的背景信息（如果可得）。\n•\t互动数据：某篇文章的点赞、收藏、评论数，可以衡量作者的影响力。\n\n用户建模\n\n基于数据构建用户画像，关键点在于：\n•\t用户兴趣分布：\n•\t关键词提取（TF-IDF、TextRank）\n•\t主题建模（LDA、BERT embedding 聚类）\n•\t预训练大模型嵌入（如 OpenAI Embedding、BGE）\n•\t专业领域分类：\n•\t基于百科、行业分类标准（如 ACM、GICS）\n•\t训练一个分类模型，给用户内容打标签\n•\t影响力/权威度：\n•\t互动数据（转发、点赞、收藏）\n•\t发表文章数量\n•\t被引用次数\n💡 示例：用户 A 画像\n领域\t关键词\t兴趣度\nAI\tTransformer, BERT, LLM\t0.8\n经济\t股票, 投资, ETF\t0.4\n运动\t健身, 跑步\t0.3\n\n\n问题建模\n•\t文本向量化：\n•\tBERT / RoBERTa / BGE embedding\n•\tTF-IDF + 主题建模（LDA）\n•\t问题分类：\n•\t训练一个多分类模型（行业分类）\n•\t规则 + 关键词匹配\n•\t难度估计：\n•\t根据问题长度、关键词专业性、历史回答情况评估（如 BM25 评分）\n\n\n匹配算法\n\n\n给定一个问题 Q，如何找到合适的回答者？\n方法 1：语义匹配\n1.\t计算相似度\n•\t计算问题 Q 的向量 Vq\n•\t计算每个用户兴趣向量 Vu\n•\t计算 cosine_similarity(Vq, Vu)\n2.\tTop-K 召回\n•\t设定相似度阈值\n•\t排序取前 K 个用户\n方法 2：基于内容推荐\n1.\t基于历史回答\n•\t查找回答过相似问题的用户\n•\t计算用户对该问题的可信度（回答质量评分）\n2.\t基于社会网络\n•\t参考微信好友/共同关注公众号\n•\t找到某个领域的“权威节点”\n💡 示例：问题 “如何训练 BERT?”\n•\t匹配用户 A（AI 方向，匹配度 0.85）\n•\t匹配用户 B（机器学习方向，匹配度 0.78）\n•\t匹配公众号 C（深度学习专栏，匹配度 0.91）\n\n交互与优化\n•\t反馈机制：用户可以对推荐的回答者打分，优化推荐策略\n•\t个性化推荐：结合用户的浏览/点赞习惯，调整排序\n•\t冷启动问题：使用外部数据（如知乎、论坛）预训练模型\n\n总结\n1.\t数据来源：从微信订阅、公众号、朋友圈等获取用户兴趣画像\n2.\t用户建模：构建基于文本分析的用户兴趣分布和影响力评分\n3.\t问题建模：提取问题关键词，生成语义向量\n4.\t匹配方法：\n•\t语义相似度（BERT embedding）\n•\t互动行为（历史回答、点赞）\n•\t社交关系（好友/关注关系）\n5.\t优化策略：反馈系统 + 个性化排序\n这个方案可以适用于类似知乎的问答平台，并结合微信生态的优势提升匹配精度。你觉得这个思路如何？可以根据你的需求进一步优化！ 🚀\n\n用户的问题可能很多样，可能没法预定义好问题类别，以及如何提高匹配的精度？\n\n你提到的问题很关键，涉及到 问题分类的泛化能力 和 匹配精度的提升。我们可以从无监督学习+语义匹配+用户权威度建模三方面优化：\n\n解决问题类别难以预定义的问题\n\n由于用户的问题可能很多样，预定义类别的方式可能不够泛化，因此可以采用 无监督/自监督方法 自动学习问题的类别或聚类关系。\n1.1 无需预定义类别的方案\n(1) 语义聚类 (Topic Modeling)\n•\t使用 BERT embedding + KMeans/HDBSCAN 对问题进行无监督聚类\n•\t结合 LDA（Latent Dirichlet Allocation） 发现潜在主题\n•\t采用 UMAP + HDBSCAN 进行高维语义降维+密度聚类，自动划分问题领域\n💡 示例\n•\tQ1：“如何微调 BERT？” -&gt; Cluster: NLP\n•\tQ2：“推荐几本深度学习的书” -&gt; Cluster: AI 教育\n•\tQ3：“ETF 和股票的区别？” -&gt; Cluster: 投资\n这样，即使没有事先定义类别，系统也能自动学习问题类别。\n(2) 语义相似度检索\n•\t无分类标签匹配：直接计算 问题与用户画像的文本相似度，而不是先分类再匹配\n•\t计算新问题与历史问题的语义距离，找到相似的问题和其回答者\n\n提高匹配精度的方法\n\n2.1 更精准的用户画像\n我们需要提升用户画像的准确性，使得匹配更加精确。优化策略如下：\n(1) 细粒度知识点提取\n•\t不是简单用 TF-IDF，而是用 Named Entity Recognition（NER）+ Dependency Parsing\n•\t例子：\n•\t“如何微调 BERT？” 提取关键词 BERT, fine-tuning\n•\t“ETF 和股票的区别？” 提取关键词 ETF, 股票\n•\t这样，我们能获取更精准的用户兴趣点。\n(2) 构建用户的动态兴趣向量\n•\t用户的兴趣是动态的，而不是固定的\n•\t时间衰减：如果用户 5 年前是 Java 专家，但最近都在关注 LLM，我们应该降低 Java 权重\n•\t解决方案：\n•\t采用时间衰减权重计算兴趣向量（Exponential Decay）\n•\t或者用 RNN/Transformer 预测兴趣变化\n(3) 结合社交关系\n•\t若用户关注某些公众号，这些公众号可能和其专业方向一致\n•\t若用户点赞、评论某些专家的文章，这些专家可能是其潜在的推荐目标\n2.2 更精准的问题理解\n(1) 语义增强\n•\t不仅使用单纯的文本相似度，而是结合外部知识库（如 Wikipedia, 知乎百科）进行扩展\n•\t用大模型（如 ChatGPT）进行 query expansion\n•\t“如何微调 BERT？” → “BERT 微调步骤, transformer 训练, NLP 预训练模型调优”\n(2) 关键词增强\n•\t使用 BERT + BM25，获取更相关的关键词，提高检索召回率\n•\t“Transformer 训练技巧” -&gt; 关联 “BERT, GPT, LLaMA 训练”\n2.3 多层次匹配\n为了提高匹配精度，我们可以结合不同层次的信息：\n1.\t语义匹配（BERT embedding 计算相似度）\n2.\t历史回答匹配（用户是否回答过类似问题）\n3.\t社交网络匹配（用户是否与某些专家互动）\n4.\t行为数据匹配（用户是否在某些领域有深度阅读/点赞）\n最终，我们可以采用 加权融合模型，根据实验调优每个匹配策略的权重。\n\n结合强化学习优化匹配\n\n最终，我们可以用 强化学习（Reinforcement Learning, RL） 进一步优化匹配：\n•\t以用户的反馈（如点赞、采纳率）作为奖励信号\n•\t用 多臂老虎机算法（Multi-Armed Bandit, MAB）动态调整推荐策略\n•\t用 强化学习 RankNet / LambdaRank 训练一个排序模型，优化匹配顺序\n总结\n1.\t无需预定义问题类别\n•\t采用 BERT embedding + 聚类（HDBSCAN/UMAP） 自动发现问题类别\n•\t采用 语义相似度检索 直接匹配相似问题\n2.\t提升匹配精度\n•\t优化用户画像（知识点提取+时间衰减）\n•\t增强问题理解（扩展查询+关键词增强）\n•\t多层次匹配（语义匹配+历史匹配+社交匹配）\n3.\t强化学习优化\n•\t基于用户反馈 进行动态优化（采纳率、点赞数等）\n这套方案可以大幅提升匹配的泛化能力和精度，你觉得怎么样？如果有具体的业务限制，我们可以再进一步调整方案！ 🚀\n","tags":["interview","场景题"]},{"title":"强化学习","url":"/2025/05/04/interview/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/","content":" PRM：基于过程的奖励模型\n ORM：基于结果的奖励模型\n","tags":["RL","ML"]},{"title":"搜广推","url":"/2025/05/04/interview/%E6%90%9C%E5%B9%BF%E6%8E%A8/","content":"\n手撕代码：三数之和\n激活函数都有那些？\nsigmoid和relu的优劣性\n梯度消失什么原因？怎么避免？\n优化器了解哪些？\nadam和adagrad有什么区别？分别适合什么场景？\n损失函数主流有哪些？为什么分类任务用ce？\n项目相关：项目中用了几层cnn，1*1的cnn有什么作用？\n对推荐链路有了解吗？分别发挥什么作用？\n精排都有哪些主流模型？\n粗排和精排的区别是什么？\n\n1.1 什么是推荐系统？\n推荐系统是一种利用数据分析、机器学习等技术，为用户提供个性化内容的系统，常见于电商、视频、音乐、社交等平台。\n1.2 推荐系统的作用？\n• 提升用户体验：帮助用户快速找到感兴趣的内容\n• 提高用户留存：增加用户粘性，提升活跃度\n• 提升转化率：提高点击率、购买率等关键指标\n• 优化内容分发：高效匹配用户与内容\n1.3 推荐系统的意义？\n• 个性化服务：不同用户看到不同的内容\n• 信息过载问题：减少用户的筛选成本\n• 商业价值：提升平台收益，如广告点击率、销售转化率\n1.4 推荐系统要解决的问题？\n• 冷启动问题（新用户、新物品缺少历史数据）\n• 数据稀疏性（用户行为数据有限，难以建模）\n• 多样性 vs. 相关性（避免推荐过于单一，保持多样性）\n• 实时性（如何快速更新推荐结果）\n• 用户隐私保护（如何在推荐中兼顾隐私）\n1.5 常用的推荐系统的逻辑框架？\n推荐系统一般包括四个层次：\n\n召回（Recall）：从海量数据中筛选出相关内容\n粗排（Pre-Ranking）：利用轻量模型（如LR、GBDT）对召回结果进行初步排序\n精排（Ranking）：用复杂模型（如深度学习）对候选集排序\n重排（Re-Ranking）：优化多样性、新颖性、用户体验\n\n1.6 常用的推荐系统的技术架构？\n推荐系统架构一般包括：\n\n数据层（日志采集、特征工程）\n算法层（召回、排序、重排）\n服务层（推荐引擎、AB测试）\n应用层（用户界面、交互反馈）\n\n1.7 推荐系统算法工程师日常解决的问题？\n• 优化召回策略（如协同过滤、内容推荐）\n• 改进排序模型（提升精排模型效果）\n• 特征工程优化（构造更有效的用户、物品特征）\n• AB测试和效果评估（调整推荐算法，提高业务指标）\n1.8 推荐系统算法工程师处理的数据部分？\n• 用户数据（点击、浏览、点赞、收藏、购买等）\n• 物品数据（商品、视频、文章等的内容信息）\n• 行为数据（用户与物品的交互，如时间、地点）\n最终得到的是用于训练推荐模型的特征数据。\n1.9 推荐系统算法工程师处理的模型部分？\n• 召回模型（协同过滤、矩阵分解、Embedding）\n• 排序模型（LR、GBDT、DeepFM、DIN、Transformer）\n最终得到的是用于预测用户偏好的推荐模型。\n1.10 模型训练的方式？\n• 离线训练（定期批量训练，更新模型）\n• 在线训练（实时学习用户行为，快速调整模型）\n• 迁移学习（用已有数据快速训练新模型）\n• 多任务学习（同时优化多个目标，如CTR和CVR）\n1.11 推荐系统的流程是什么？\n\n数据采集（用户行为日志、内容数据）\n特征工程（构造用户、物品、交互特征）\n召回（初步筛选候选集）\n排序（根据模型分数排序）\n重排（优化用户体验，如多样性）\n推荐展示（给用户提供推荐列表）\n反馈优化（用户点击、跳出率等数据用于优化推荐）\n\n1.12 推荐系统与搜索、广告的异同？\n\n\n\n维度\n推荐系统\n搜索\n广告\n\n\n\n\n用户输入\n无需输入，系统主动推荐\n用户输入关键词\n依据广告竞价和用户画像\n\n\n核心目标\n提高用户体验，增加粘性\n帮助用户找到目标内容\n提高广告转化率\n\n\n算法原理\n召回 + 排序\n检索 + 排序\n竞价 + 排序\n\n\n业务场景\n视频、电商、社交\n购物、知识问答\n电商、内容平台、搜索引擎\n\n\n\n1.13 推荐系统整体架构？\n\n数据层（日志、特征工程）\n召回层（用户行为、物品匹配）\n粗排层（轻量级排序模型）\n精排层（复杂深度学习模型）\n重排层（多样性、探索机制）\n在线服务（推荐 API、AB 测试、日志反馈）\n\n","tags":["interview","搜广推"]}]