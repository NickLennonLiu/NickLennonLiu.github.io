<!doctype html><html lang=zh-CN><meta charset=UTF-8><meta content="width=device-width" name=viewport><meta content=#222 name=theme-color><meta content="Hexo 7.3.0" name=generator><link crossorigin href=https://cdnjs.cloudflare.com rel=preconnect><link href=/favicon/apple-touch-icon.png rel=apple-touch-icon sizes=180x180><link href=/favicon/web-app-manifest-512x512.png rel=icon sizes=32x32 type=image/png><link href=/favicon/web-app-manifest-192x192.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/favicon/favicon.svg rel=mask-icon><meta content=dtILjtKPbqBNIxMl4PiZQihoHIODQU8NU2t2eulsvXI name=google-site-verification><meta content=codeva-7UaWTTz0pX name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css rel=stylesheet><link integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin href=https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css rel=stylesheet><link integrity="sha256-zM8WXtG4eUn7dKKNMTuoWZub++VnSfaOpA/8PJfvTBo=" crossorigin href=https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.36/fancybox/fancybox.css rel=stylesheet><link href=https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css rel=stylesheet><script integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin defer src=https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js></script><script class=next-config data-name=main type=application/json>{"hostname":"blog.river9.top","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.23.0","exturl":true,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"flat"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.json","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script defer src=/js/config.js></script><meta content="[toc]  DeepSeek [[DeepSeek]]  拒绝采样 [[概率论#LLM 的拒绝采样]]  显存计算  推理  int8： 模型显存 = 1*参数量（Byte） fp16, bf16: 模型显存 = 2*参数量（Byte） fp32: 模型显存 = 4*参数量（Byte）  混合精度（Mixed-precision）最后存储的类型也是fp32，公式也适合混合精度。  训练 训练显存" name=description><meta content=article property=og:type><meta content=大模型 property=og:title><meta content=https://blog.river9.top/2025/05/04/interview/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B8%B8%E8%A7%81%E9%9D%A2%E7%BB%8F/index.html property=og:url><meta content="Junetheriver's Blog" property=og:site_name><meta content="[toc]  DeepSeek [[DeepSeek]]  拒绝采样 [[概率论#LLM 的拒绝采样]]  显存计算  推理  int8： 模型显存 = 1*参数量（Byte） fp16, bf16: 模型显存 = 2*参数量（Byte） fp32: 模型显存 = 4*参数量（Byte）  混合精度（Mixed-precision）最后存储的类型也是fp32，公式也适合混合精度。  训练 训练显存" property=og:description><meta content=zh_CN property=og:locale><meta content=2025-05-03T19:04:40.423Z property=article:published_time><meta content=2025-05-04T03:12:08.538Z property=article:modified_time><meta content="River June" property=article:author><meta content=llm property=article:tag><meta content=summary name=twitter:card><link href=https://blog.river9.top/2025/05/04/interview/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B8%B8%E8%A7%81%E9%9D%A2%E7%BB%8F/ rel=canonical><script class=next-config data-name=page type=application/json>{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://blog.river9.top/2025/05/04/interview/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B8%B8%E8%A7%81%E9%9D%A2%E7%BB%8F/","path":"2025/05/04/interview/大模型常见面经/","title":"大模型"}</script><script class=next-config data-name=calendar type=application/json>""</script><title>大模型 | Junetheriver's Blog</title><script alpha=0.6 defer size=600 src=https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js zindex=-1></script><script integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin defer src=https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js></script><script integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin defer src=https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js></script><script integrity="sha256-hiUEBwFEpLF6DlB8sGXlKo4kPZ46Ui4qGpd0vrVkOm4=" crossorigin defer src=https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.36/fancybox/fancybox.umd.js></script><script integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin defer src=https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js></script><script integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin defer src=https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js></script><script defer src=/js/utils.js></script><script defer src=/js/motion.js></script><script defer src=/js/sidebar.js></script><script defer src=/js/next-boot.js></script><script defer src=/js/pjax.js></script><script integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin defer src=https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js></script><script defer src=/js/third-party/search/local-search.js></script><script class=next-config data-name=pdf type=application/json>{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.3.1/pdfobject.min.js","integrity":"sha256-jI72I8ZLVflVOisZIOaLvRew3tyvzeu6aZXFm7P7dEo="},"url":"/lib/pdf/web/viewer.html"}</script><script defer src=/js/third-party/tags/pdf.js></script><script class=next-config data-name=mermaid type=application/json>{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.5.0/mermaid.min.js","integrity":"sha256-2obLuIPcceEhkE3G09G33hBdmE55ivVcZUlcKcGNHjU="}}</script><script defer src=/js/third-party/tags/mermaid.js></script><script defer src=/js/third-party/fancybox.js></script><script defer src=/js/third-party/pace.js></script><script defer src=/js/third-party/addtoany.js></script><script class=next-config data-name=enableMath type=application/json>false</script><link integrity="sha256-UF1fgpAiu3tPJN/uCqEUHNe7pnr+QR0SQDNfgglgtcM=" crossorigin href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css rel=stylesheet><script class=next-config data-name=katex type=application/json>{"copy_tex_js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/copy-tex.min.js","integrity":"sha256-Us54+rSGDSTvIhKKUs4kygE2ipA0RXpWWh0/zLqw3bs="}}</script><script defer src=/js/third-party/math/katex.js></script><script integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin defer src=https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js></script><script class=next-config data-name=quicklink type=application/json>{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"https://blog.river9.top/2025/05/04/interview/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B8%B8%E8%A7%81%E9%9D%A2%E7%BB%8F/"}</script><script defer src=/js/third-party/quicklink.js></script><script class=next-config data-name=exif type=application/json>"{FocalLength}mm f/{ApertureValue} {ExposureTime}s"</script><script defer src=https://cdn.jsdelivr.net/npm/exifreader@4/dist/exif-reader.min.js></script><script defer src=/lib/exif.js></script><noscript><link href=/css/noscript.css rel=stylesheet></noscript><body class=use-motion itemscope itemtype=http://schema.org/WebPage><div class=headband></div><main class=main><div class=column><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle role=button><span class=toggle-line></span><span class=toggle-line></span><span class=toggle-line></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <i class=logo-line></i> <p class=site-title>Junetheriver's Blog</p> <i class=logo-line></i> </a></div><div class=site-nav-right><div class="toggle popup-trigger" aria-label=搜索 role=button><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签<span class=badge>11</span></a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类<span class=badge>1</span></a><li class="menu-item menu-item-roadmap"><a href=/roadmap/ rel=section><i class="fa fa-road fa-fw"></i>Roadmap</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container><input autocapitalize=off autocomplete=off class=search-input maxlength=80 placeholder=搜索... spellcheck=false type=search></div><span class=popup-btn-close role=button> <i class="fa fa-times-circle"></i> </span></div><div class=search-result-container><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></header><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><div class=sidebar-panel-container><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class=nav><li class="nav-item nav-level-2"><a class=nav-link href=#deepseek><span class=nav-number>1.</span> <span class=nav-text> DeepSeek</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#%E6%8B%92%E7%BB%9D%E9%87%87%E6%A0%B7><span class=nav-number>2.</span> <span class=nav-text> 拒绝采样</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#%E6%98%BE%E5%AD%98%E8%AE%A1%E7%AE%97><span class=nav-number>3.</span> <span class=nav-text> 显存计算</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#%E6%8E%A8%E7%90%86><span class=nav-number>3.1.</span> <span class=nav-text> 推理</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E8%AE%AD%E7%BB%83><span class=nav-number>3.2.</span> <span class=nav-text> 训练</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0><span class=nav-number>3.2.1.</span> <span class=nav-text> 模型参数</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E4%BC%98%E5%8C%96%E5%99%A8%E7%8A%B6%E6%80%81><span class=nav-number>3.2.2.</span> <span class=nav-text> 优化器状态</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%A2%AF%E5%BA%A6%E5%80%BC><span class=nav-number>3.2.3.</span> <span class=nav-text> 梯度值</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%BF%80%E6%B4%BB%E5%80%BC><span class=nav-number>3.2.4.</span> <span class=nav-text> 激活值</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%8D%95%E5%8D%A1%E4%B8%8E%E5%B9%B6%E8%A1%8C><span class=nav-number>3.2.5.</span> <span class=nav-text> 单卡与并行</span></a></ol></ol><li class="nav-item nav-level-2"><a class=nav-link href=#lora><span class=nav-number>4.</span> <span class=nav-text> LORA</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#stable-diffusion%E5%8E%9F%E7%90%86><span class=nav-number>5.</span> <span class=nav-text> Stable Diffusion原理</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84><span class=nav-number>6.</span> <span class=nav-text> 大模型的模型架构</span></a></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author animated" itemprop=author itemscope itemtype=http://schema.org/Person><img alt="River June" class=site-author-image itemprop=image src=/favicon/cirno_pat.gif><p class=site-author-name itemprop=name>River June<div class=site-description itemprop=description>An Idiot to Anything</div></div><div class="links-of-author animated"><span class=links-of-author-item> <span data-url="aHR0cHM6Ly9naXRodWIuY29tL05pY2tMZW5ub25MaXU=" title="GitHub → https://github.com/NickLennonLiu" class=exturl><i class="fab fa-github fa-fw"></i>GitHub</span> </span><span class=links-of-author-item> <span data-url="bWFpbHRvOmp1bmV0aGVyaXZlckBnbWFpbC5jb20=" title="E-Mail → mailto:junetheriver@gmail.com" class=exturl><i class="fa fa-envelope fa-fw"></i>E-Mail</span> </span><span class=links-of-author-item> <span title="Bilibili → https://space.bilibili.com/800711" class=exturl data-url=aHR0cHM6Ly9zcGFjZS5iaWxpYmlsaS5jb20vODAwNzEx><i class="fa fa-television fa-fw"></i>Bilibili</span> </span></div></div></div></div><div class=pjax></div></aside></div><div class="main-inner post posts-expand"><div class=post-block><article class=post-content itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://blog.river9.top/2025/05/04/interview/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B8%B8%E8%A7%81%E9%9D%A2%E7%BB%8F/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=/favicon/cirno_pat.gif itemprop=image> <meta content="River June" itemprop=name> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content="Junetheriver's Blog" itemprop=name> <meta content="An Idiot to Anything" itemprop=description> </span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork> <meta content="大模型 | Junetheriver's Blog" itemprop=name> <meta itemprop=description> </span><header class=post-header><h1 itemprop="name headline" class=post-title>大模型</h1><div class=post-meta-container><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2025-05-04 03:04:40 / 修改时间：11:12:08" datetime=2025-05-04T03:04:40+08:00>2025-05-04</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-folder"></i> </span> <span class=post-meta-item-text>分类于</span> <span itemprop=about itemscope itemtype=http://schema.org/Thing> <a href=/categories/interview/ itemprop=url rel=index><span itemprop=name>interview</span></a> </span> </span><span class=post-meta-break></span><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>627</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>2 分钟</span> </span></div></div></header><div class=post-body itemprop=articleBody><p>[toc]<h2 id=deepseek><a class=markdownIt-Anchor href=#deepseek></a> DeepSeek</h2><p>[[DeepSeek]]<h2 id=拒绝采样><a class=markdownIt-Anchor href=#拒绝采样></a> 拒绝采样</h2><p>[[概率论#LLM 的拒绝采样]]<h2 id=显存计算><a class=markdownIt-Anchor href=#显存计算></a> 显存计算</h2><h3 id=推理><a class=markdownIt-Anchor href=#推理></a> 推理</h3><ul><li>int8： 模型显存 = 1*参数量（Byte）<li>fp16, bf16: 模型显存 = 2*参数量（Byte）<li>fp32: 模型显存 = 4*参数量（Byte）</ul><p>混合精度（Mixed-precision）最后存储的类型也是fp32，公式也适合混合精度。<h3 id=训练><a class=markdownIt-Anchor href=#训练></a> 训练</h3><p>训练显存消耗：模型参数(Model) + 优化器状态(Optimizer status) + 梯度 (Gradient) + 激活值 (Activation)<p>静态值：模型参数和优化器状态<br> 动态值：激活值和梯度值<p>下面默认参数都是4Byte<h4 id=模型参数><a class=markdownIt-Anchor href=#模型参数></a> 模型参数</h4><p>同推理<h4 id=优化器状态><a class=markdownIt-Anchor href=#优化器状态></a> 优化器状态</h4><p>常见优化器Adam：Momentum和一个Variance，混合精度训练中再加一个模型参数副本<ul><li>模型副本 4Bytes （副本存32位的）<li>Momentum 4Byte<li>Variance 4Byte</ul><p>那就是参数副本+momentum+variance = 32 * 3<h4 id=梯度值><a class=markdownIt-Anchor href=#梯度值></a> 梯度值</h4><p>梯度的数据类型和模型参数一直，因此和模型参数同样规模<h4 id=激活值><a class=markdownIt-Anchor href=#激活值></a> 激活值</h4><p>激活值的大小跟模型参数、重计算、并行策略等相关，这里我们参考Megtron论文里面给的计算公式，来求解激活值所占用的显存大小。<p class=katex-block><span class=katex-display><span class=katex><span class=katex-mathml><math display=block xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>s</mi><mo>∗</mo><mi>b</mi><mo>∗</mo><mi>h</mi><mo>∗</mo><mo stretchy=false>(</mo><mn>34</mn><mo>+</mo><mn>5</mn><mo>∗</mo><mi>a</mi><mo>∗</mo><mi>s</mi><mi mathvariant=normal>/</mi><mi>h</mi><mo stretchy=false>)</mo><mo>∗</mo><mi>L</mi><mo>∗</mo><mi>γ</mi></mrow><annotation encoding=application/x-tex>s*b*h*(34+5*a*s/h) *L *\gamma </annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.46528em;vertical-align:0em;></span><span class="mord mathnormal">s</span><span class=mspace style=margin-right:0.2222222222222222em;></span><span class=mbin>∗</span><span class=mspace style=margin-right:0.2222222222222222em;></span></span><span class=base><span class=strut style=height:0.69444em;vertical-align:0em;></span><span class="mord mathnormal">b</span><span class=mspace style=margin-right:0.2222222222222222em;></span><span class=mbin>∗</span><span class=mspace style=margin-right:0.2222222222222222em;></span></span><span class=base><span class=strut style=height:0.69444em;vertical-align:0em;></span><span class="mord mathnormal">h</span><span class=mspace style=margin-right:0.2222222222222222em;></span><span class=mbin>∗</span><span class=mspace style=margin-right:0.2222222222222222em;></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em;></span><span class=mopen>(</span><span class=mord>3</span><span class=mord>4</span><span class=mspace style=margin-right:0.2222222222222222em;></span><span class=mbin>+</span><span class=mspace style=margin-right:0.2222222222222222em;></span></span><span class=base><span class=strut style=height:0.64444em;vertical-align:0em;></span><span class=mord>5</span><span class=mspace style=margin-right:0.2222222222222222em;></span><span class=mbin>∗</span><span class=mspace style=margin-right:0.2222222222222222em;></span></span><span class=base><span class=strut style=height:0.46528em;vertical-align:0em;></span><span class="mord mathnormal">a</span><span class=mspace style=margin-right:0.2222222222222222em;></span><span class=mbin>∗</span><span class=mspace style=margin-right:0.2222222222222222em;></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em;></span><span class="mord mathnormal">s</span><span class=mord>/</span><span class="mord mathnormal">h</span><span class=mclose>)</span><span class=mspace style=margin-right:0.2222222222222222em;></span><span class=mbin>∗</span><span class=mspace style=margin-right:0.2222222222222222em;></span></span><span class=base><span class=strut style=height:0.68333em;vertical-align:0em;></span><span class="mord mathnormal">L</span><span class=mspace style=margin-right:0.2222222222222222em;></span><span class=mbin>∗</span><span class=mspace style=margin-right:0.2222222222222222em;></span></span><span class=base><span class=strut style=height:0.625em;vertical-align:-0.19444em;></span><span class="mord mathnormal" style=margin-right:0.05556em;>γ</span></span></span></span></span><ul><li>s 序列长度（sequence length), tokens的量<li>b 微批量大小（microbatch size）<li>h 隐藏层大小（hidden dimension size）<li>a attention的头数 （number of attention heads）<li>L transformer模型的层数<li>λ 比例系数，当为fp16时 值等于1 / (1024 * 1024 * 1024)。</ul><p>如果全部重计算：<p class=katex-block><span class=katex-display><span class=katex><span class=katex-mathml><math display=block xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>s</mi><mo>∗</mo><mi>b</mi><mo>∗</mo><mi>h</mi><mo>∗</mo><mo stretchy=false>(</mo><mn>2</mn><mo stretchy=false>)</mo><mo>∗</mo><mi>L</mi><mo>∗</mo><mi>γ</mi></mrow><annotation encoding=application/x-tex>s*b*h*(2)*L*\gamma </annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.46528em;vertical-align:0em;></span><span class="mord mathnormal">s</span><span class=mspace style=margin-right:0.2222222222222222em;></span><span class=mbin>∗</span><span class=mspace style=margin-right:0.2222222222222222em;></span></span><span class=base><span class=strut style=height:0.69444em;vertical-align:0em;></span><span class="mord mathnormal">b</span><span class=mspace style=margin-right:0.2222222222222222em;></span><span class=mbin>∗</span><span class=mspace style=margin-right:0.2222222222222222em;></span></span><span class=base><span class=strut style=height:0.69444em;vertical-align:0em;></span><span class="mord mathnormal">h</span><span class=mspace style=margin-right:0.2222222222222222em;></span><span class=mbin>∗</span><span class=mspace style=margin-right:0.2222222222222222em;></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em;></span><span class=mopen>(</span><span class=mord>2</span><span class=mclose>)</span><span class=mspace style=margin-right:0.2222222222222222em;></span><span class=mbin>∗</span><span class=mspace style=margin-right:0.2222222222222222em;></span></span><span class=base><span class=strut style=height:0.68333em;vertical-align:0em;></span><span class="mord mathnormal">L</span><span class=mspace style=margin-right:0.2222222222222222em;></span><span class=mbin>∗</span><span class=mspace style=margin-right:0.2222222222222222em;></span></span><span class=base><span class=strut style=height:0.625em;vertical-align:-0.19444em;></span><span class="mord mathnormal" style=margin-right:0.05556em;>γ</span></span></span></span></span><h4 id=单卡与并行><a class=markdownIt-Anchor href=#单卡与并行></a> 单卡与并行</h4><p class=katex-block><span class=katex-display><span class=katex><span class=katex-mathml><math display=block xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mtext>TotalMemory</mtext><mo>=</mo><mtext>Model</mtext><mo>+</mo><mtext>Optimizer</mtext><mo>+</mo><mtext>Activation</mtext><mo>+</mo><mtext>Gradient</mtext></mrow><annotation encoding=application/x-tex>\text{TotalMemory} = \text{Model} + \text{Optimizer} + \text{Activation} + \text{Gradient} </annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.8888799999999999em;vertical-align:-0.19444em;></span><span class="mord text"><span class=mord>TotalMemory</span></span><span class=mspace style=margin-right:0.2777777777777778em;></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2777777777777778em;></span></span><span class=base><span class=strut style=height:0.77777em;vertical-align:-0.08333em;></span><span class="mord text"><span class=mord>Model</span></span><span class=mspace style=margin-right:0.2222222222222222em;></span><span class=mbin>+</span><span class=mspace style=margin-right:0.2222222222222222em;></span></span><span class=base><span class=strut style=height:0.8777699999999999em;vertical-align:-0.19444em;></span><span class="mord text"><span class=mord>Optimizer</span></span><span class=mspace style=margin-right:0.2222222222222222em;></span><span class=mbin>+</span><span class=mspace style=margin-right:0.2222222222222222em;></span></span><span class=base><span class=strut style=height:0.76666em;vertical-align:-0.08333em;></span><span class="mord text"><span class=mord>Activation</span></span><span class=mspace style=margin-right:0.2222222222222222em;></span><span class=mbin>+</span><span class=mspace style=margin-right:0.2222222222222222em;></span></span><span class=base><span class=strut style=height:0.69444em;vertical-align:0em;></span><span class="mord text"><span class=mord>Gradient</span></span></span></span></span></span><ul><li>TP: tensor并行：Megatron-LM<li>SP: 序列并行<li>PP: pipeline并行<li>Zero：参数服务器 <ul><li>Zero1：优化器参数分散到N张卡<li>Zero2：优化器和梯度分散到N张卡<li>Zero3：优化器、梯度、模型都分散到N张卡，并且还要算上新引入的参数LiveParams，用于决定哪些参数加载到GPU中</ul></ul><p class=katex-block><span class=katex-display><span class=katex><span class=katex-mathml><math display=block xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mtext>TotalMemory</mtext><mo>=</mo><mtext>Model</mtext><mi mathvariant=normal>/</mi><mo stretchy=false>(</mo><mi>P</mi><mi>P</mi><mo>∗</mo><mi>T</mi><mi>P</mi><mo stretchy=false>)</mo><mo>+</mo><mtext>Optimizer</mtext><mi mathvariant=normal>/</mi><mi>N</mi><mo>+</mo><mtext>Activation</mtext><mi mathvariant=normal>/</mi><mi>T</mi><mi>P</mi><mo>+</mo><mtext>Gradient</mtext><mi mathvariant=normal>/</mi><mi>P</mi><mi>P</mi></mrow><annotation encoding=application/x-tex>\text{TotalMemory} = \text{Model}/(PP*TP) + \text{Optimizer}/N + \text{Activation}/TP + \text{Gradient}/PP </annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.8888799999999999em;vertical-align:-0.19444em;></span><span class="mord text"><span class=mord>TotalMemory</span></span><span class=mspace style=margin-right:0.2777777777777778em;></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2777777777777778em;></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em;></span><span class="mord text"><span class=mord>Model</span></span><span class=mord>/</span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:0.13889em;>P</span><span class="mord mathnormal" style=margin-right:0.13889em;>P</span><span class=mspace style=margin-right:0.2222222222222222em;></span><span class=mbin>∗</span><span class=mspace style=margin-right:0.2222222222222222em;></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em;></span><span class="mord mathnormal" style=margin-right:0.13889em;>T</span><span class="mord mathnormal" style=margin-right:0.13889em;>P</span><span class=mclose>)</span><span class=mspace style=margin-right:0.2222222222222222em;></span><span class=mbin>+</span><span class=mspace style=margin-right:0.2222222222222222em;></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em;></span><span class="mord text"><span class=mord>Optimizer</span></span><span class=mord>/</span><span class="mord mathnormal" style=margin-right:0.10903em;>N</span><span class=mspace style=margin-right:0.2222222222222222em;></span><span class=mbin>+</span><span class=mspace style=margin-right:0.2222222222222222em;></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em;></span><span class="mord text"><span class=mord>Activation</span></span><span class=mord>/</span><span class="mord mathnormal" style=margin-right:0.13889em;>T</span><span class="mord mathnormal" style=margin-right:0.13889em;>P</span><span class=mspace style=margin-right:0.2222222222222222em;></span><span class=mbin>+</span><span class=mspace style=margin-right:0.2222222222222222em;></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em;></span><span class="mord text"><span class=mord>Gradient</span></span><span class=mord>/</span><span class="mord mathnormal" style=margin-right:0.13889em;>P</span><span class="mord mathnormal" style=margin-right:0.13889em;>P</span></span></span></span></span><h2 id=lora><a class=markdownIt-Anchor href=#lora></a> LORA</h2><p>Lora方法的核心是在大型语言模型上对指定参数增加额外的低秩矩阵，也就是在原始PLM旁边增加一个旁路，做一个降维再升维的操作。并在模型训练过程中，固定PLM的参数，只训练降维矩阵A与升维矩阵B。<p>Ptuning方法的核心是使用可微的virtual token替换了原来的discrete tokens，且仅加入到输入层，并使用prompt<h2 id=stable-diffusion原理><a class=markdownIt-Anchor href=#stable-diffusion原理></a> Stable Diffusion原理</h2><p>[[Stable Diffusion]]<ul><li>TextEncoder <ul><li>CLIP</ul><li>Diffusion<li>VAE</ul><h2 id=大模型的模型架构><a class=markdownIt-Anchor href=#大模型的模型架构></a> 大模型的模型架构</h2><ul><li>Only-encoder<li>Only-decoder<li>Encoder-Decoder</ul></div><footer class=post-footer><div class=post-copyright><ul><li class=post-copyright-author><strong>本文作者： </strong>River June<li class=post-copyright-link><strong>本文链接：</strong> <a href=https://blog.river9.top/2025/05/04/interview/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B8%B8%E8%A7%81%E9%9D%A2%E7%BB%8F/ title=大模型>https://blog.river9.top/2025/05/04/interview/大模型常见面经/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <span data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8=" class=exturl><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</span> 许可协议。转载请注明出处！</ul></div><div class=post-tags><a href=/tags/llm/ rel=tag><i class="fa fa-tag"></i> llm</a></div><div class="social-like a2a_kit a2a_kit_size_32 a2a_default_style"><a class=a2a_dd href=https://www.addtoany.com/share rel=noopener target=_blank></a><a class=a2a_button_facebook></a><a class=a2a_button_twitter></a><a class=a2a_button_telegram></a><a class=a2a_button_wechat></a></div><div class=post-nav><div class=post-nav-item><a href=/2025/05/04/interview/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/ rel=prev title=强化学习> <i class="fa fa-angle-left"></i> 强化学习 </a></div><div class=post-nav-item><a href=/2025/05/04/interview/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ rel=next title=机器学习> 机器学习 <i class="fa fa-angle-right"></i> </a></div></div></footer></article></div><div class=comments id=disqus_thread><noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© <span itemprop=copyrightYear>2025</span><span class=with-love> <i class="fa fa-9"></i> </span><span class=author itemprop=copyrightHolder>River June</span></div><div class=wordcount><span class=post-meta-item> <span class=post-meta-item-icon> <i class="fa fa-chart-line"></i> </span> <span title=站点总字数>13k</span> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span> <span title=站点阅读时长>48 分钟</span> </span></div><div class=powered-by>由 <span class=exturl data-url=aHR0cHM6Ly9oZXhvLmlv>Hexo</span> & <span data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZw==" class=exturl>NexT.Gemini</span> 强力驱动</div></div></footer><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span><span class=toggle-line></span><span class=toggle-line></span></div><div class=sidebar-dimmer></div><div aria-label=返回顶部 class=back-to-top role=button><i class="fa fa-arrow-up fa-lg"></i><span>0%</span></div><div class=reading-progress-bar></div><span aria-label="在 GitHub 上关注我" class="exturl github-corner" data-url="aHR0cHM6Ly9naXRodWIuY29tL05pY2tMZW5ub25MaXU=" title="在 GitHub 上关注我"><svg viewbox="0 0 250 250" aria-hidden=true height=80 width=80><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" style="transform-origin: 130px 106px;" class=octo-arm fill=currentColor></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" class=octo-body fill=currentColor></path></svg></span><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><script class=next-config data-name=disqus type=application/json>{"enable":true,"shortname":"blog-river9","count":false,"i18n":{"disqus":"disqus"}}</script><script defer src=/js/third-party/comments/disqus.js></script>